{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "CLODSA_Nuclei.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxQekSqTht6I"
   },
   "source": [
    "Sources:\n",
    "https://github.com/joheras/CLoDSA"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Augmenting a dataset for instance segmentation\n",
    "\n",
    "In this notebook, we illustrate how CLODSA can be employed to augment a dataset of images devoted to instance segmentation that was annotated using the [COCO format](http://cocodataset.org/#home). \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaBilQHUht6u"
   },
   "source": [
    "## Augmentation techniques\n",
    "\n",
    "For this example, we consider three augmentation techniques. \n",
    "\n",
    "The augmentation techniques applied in this example are:\n",
    "- Rotation.\n",
    "- Flip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCILufF2ht6y"
   },
   "source": [
    "## Installing the necessary libraries\n",
    "\n",
    "In case that CLODSA is not installed in your system, the first task consists in installing it using ``pip``."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FWTm8dG3ht6y"
   },
   "source": [
    "!pip install clodsa"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting clodsa\r\n",
      "  Downloading clodsa-1.2.47.tar.gz (30 kB)\r\n",
      "Collecting Keras\r\n",
      "  Using cached keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\r\n",
      "Collecting commentjson\r\n",
      "  Downloading commentjson-0.9.0.tar.gz (8.7 kB)\r\n",
      "Collecting h5py\r\n",
      "  Downloading h5py-3.6.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 4.5 MB 2.5 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting imutils\r\n",
      "  Downloading imutils-0.5.4.tar.gz (17 kB)\r\n",
      "Requirement already satisfied: joblib in ./VENV/lib/python3.8/site-packages (from clodsa) (1.1.0)\r\n",
      "Collecting mahotas\r\n",
      "  Downloading mahotas-1.4.12-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.5 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 5.5 MB 17.7 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: numpy in ./VENV/lib/python3.8/site-packages (from clodsa) (1.21.4)\r\n",
      "Collecting progressbar2\r\n",
      "  Downloading progressbar2-3.55.0-py2.py3-none-any.whl (26 kB)\r\n",
      "Requirement already satisfied: scikit_learn in ./VENV/lib/python3.8/site-packages (from clodsa) (1.0.1)\r\n",
      "Requirement already satisfied: scipy in ./VENV/lib/python3.8/site-packages (from clodsa) (1.7.3)\r\n",
      "Collecting lark-parser<0.8.0,>=0.7.1\r\n",
      "  Downloading lark-parser-0.7.8.tar.gz (276 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 276 kB 59.3 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting python-utils>=2.3.0\r\n",
      "  Downloading python_utils-2.5.6-py2.py3-none-any.whl (12 kB)\r\n",
      "Requirement already satisfied: six in ./VENV/lib/python3.8/site-packages (from progressbar2->clodsa) (1.16.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./VENV/lib/python3.8/site-packages (from scikit_learn->clodsa) (3.0.0)\r\n",
      "Building wheels for collected packages: clodsa, commentjson, imutils, lark-parser\r\n",
      "  Building wheel for clodsa (setup.py) ... \u001B[?25l-\b \bdone\r\n",
      "\u001B[?25h  Created wheel for clodsa: filename=clodsa-1.2.47-py2.py3-none-any.whl size=74295 sha256=918d081a152c6e8e53447b8671fc72761359aabb475aa3b4f3c0d571b832e283\r\n",
      "  Stored in directory: /home/smallfish/.cache/pip/wheels/d6/18/18/43175c7acb14572e1c5ecf19ce52abe8dfee5d9286d36c5c15\r\n",
      "  Building wheel for commentjson (setup.py) ... \u001B[?25l-\b \bdone\r\n",
      "\u001B[?25h  Created wheel for commentjson: filename=commentjson-0.9.0-py3-none-any.whl size=12084 sha256=bf3b569d75fd96a0fff8271f1c0f702f4cba06e82b3bbb82eb13eb049fd09fb4\r\n",
      "  Stored in directory: /home/smallfish/.cache/pip/wheels/2c/1c/b5/6f1b1411615716f6d2b52b9301bfaf032ed5f68d4c7d547be8\r\n",
      "  Building wheel for imutils (setup.py) ... \u001B[?25l-\b \bdone\r\n",
      "\u001B[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25859 sha256=5d816e2c56946d3f6a6720ef94ec1f270289c2fd4a53ef351c3d815b82c5aeb4\r\n",
      "  Stored in directory: /home/smallfish/.cache/pip/wheels/59/1b/52/0dea905f8278d5514dc4d0be5e251967f8681670cadd3dca89\r\n",
      "  Building wheel for lark-parser (setup.py) ... \u001B[?25l-\b \bdone\r\n",
      "\u001B[?25h  Created wheel for lark-parser: filename=lark_parser-0.7.8-py2.py3-none-any.whl size=62514 sha256=b3b3e17e3b18de77fb6af8593064d6dcad00dc78a9af60e013808e51f74d3ca6\r\n",
      "  Stored in directory: /home/smallfish/.cache/pip/wheels/11/b5/2b/b6896f25d9b272b4f72db3a45a15cb0b7a6e43d7980c936a15\r\n",
      "Successfully built clodsa commentjson imutils lark-parser\r\n",
      "Installing collected packages: Keras, lark-parser, commentjson, h5py, imutils, mahotas, python-utils, progressbar2, clodsa\r\n",
      "Successfully installed Keras-2.7.0 clodsa-1.2.47 commentjson-0.9.0 h5py-3.6.0 imutils-0.5.4 lark-parser-0.7.8 mahotas-1.4.12 progressbar2-3.55.0 python-utils-2.5.6\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1q3x_OFht66"
   },
   "source": [
    "## Loading the necessary libraries\n",
    "\n",
    "The first step in the pipeline consists in loading the necessary libraries to apply the data augmentation techniques in CLODSA."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JqWBswFyht68"
   },
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from clodsa.augmentors.augmentorFactory import createAugmentor\n",
    "from clodsa.transformers.transformerFactory import transformerGenerator\n",
    "from clodsa.techniques.techniqueFactory import createTechnique\n",
    "import cv2\n",
    "%matplotlib inline"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBP59dqqht7E"
   },
   "source": [
    "## Creating the augmentor object\n",
    "\n",
    "As explained in the documentation of CLODSA, we need to specify some parameters for the augmentation process, and use them to create an augmentor object.  \n",
    "\n",
    "_The kind of problem_. In this case, we are working in an instance segmentation problem."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zQ5q8WVnht7G"
   },
   "source": [
    "PROBLEM = \"instance_segmentation\""
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D46gdf-4ht7K"
   },
   "source": [
    "_The annotation mode_. The annotation is provided using the coco format in a file called annotations.json. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rrlRg-FVht7M"
   },
   "source": [
    "ANNOTATION_MODE = \"coco\""
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cn-uF33Oht7S"
   },
   "source": [
    "_The input path_. The input path containing the images. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "78jPXCj2ht7U"
   },
   "source": [
    "INPUT_PATH = \"dataset/train_images\""
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9kGGhs4ht7a"
   },
   "source": [
    "_The generation mode_. In this case, linear, that is, all the augmentation techniques are applied to all the images of the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KCweCzLeht7c"
   },
   "source": [
    "GENERATION_MODE = \"linear\""
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6WljljVht7g"
   },
   "source": [
    "_The output mode_. The generated images will be stored in a new folder called output.  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A4uKKcJUht7i"
   },
   "source": [
    "OUTPUT_MODE = \"coco\"\n",
    "OUTPUT_PATH= \"aug/\""
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h_eCHyI6CGPX"
   },
   "source": [
    "!mkdir aug"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9R79LEvVht7o"
   },
   "source": [
    "Using the above information, we can create our augmentor object. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CQ9wyiQuht7q"
   },
   "source": [
    "augmentor = createAugmentor(PROBLEM,ANNOTATION_MODE,OUTPUT_MODE,GENERATION_MODE,INPUT_PATH,{\"outputPath\":OUTPUT_PATH})"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXOfuq90ht7w"
   },
   "source": [
    "## Adding the augmentation techniques\n",
    "\n",
    "Now, we define the techniques that will be applied in our augmentation process and add them to our augmentor object. To illustrate the transformations, we will use the following image of the dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MsVfe3CBh1I"
   },
   "source": [
    "First of all, we must define a transformer generator."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-OSl47BDBh1K"
   },
   "source": [
    "transformer = transformerGenerator(PROBLEM)"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4PfSKW-ht74"
   },
   "source": [
    "_Rotations:_"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ajKE-mkDht74"
   },
   "source": [
    "for angle in [90,180]:\n",
    "    rotate = createTechnique(\"rotate\", {\"angle\" : angle})\n",
    "    augmentor.addTransformer(transformer(rotate))"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "It-_OkKfBh1e"
   },
   "source": [
    "_Flips:_"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gEYJCW0aBh1g"
   },
   "source": [
    "flip = createTechnique(\"flip\",{\"flip\":1})\n",
    "augmentor.addTransformer(transformer(flip))"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwE-qSYLht9I"
   },
   "source": [
    "## Applying the augmentation process\n",
    "\n",
    "Finally, we apply the augmentation process (this might take some time depending on the number of images of the original dataset and the number of transformations that will be applied). "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lbW5YVE9ht9I"
   },
   "source": [
    "!cp dataset/nuclei_train_dataset.json dataset/train_images/annotations.json"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "augmentor.applyAugmentation()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brG8bP-RBh1q"
   },
   "source": [
    "We can now check the elements of the output folder. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1zdxLh12Bh1s",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!ls aug/"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}